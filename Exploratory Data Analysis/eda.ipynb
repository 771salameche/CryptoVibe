{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e173567",
   "metadata": {},
   "source": [
    " # Analyse Exploratoire (EDA) - Posts Crypto (Reddit & Twitter)\n",
    "\n",
    " **Objectif:** Analyser un dataset consolid√© de posts Reddit et Twitter pour comprendre les tendances, la qualit√© des donn√©es et les sujets de discussion.\n",
    "\n",
    " **Fichier:** `consolidated_data.csv`\n",
    " **Colonnes:** `id`, `text`, `date`, `source`, `author`, `engagement`, `crypto_mentioned`, `date_only`\n",
    "\n",
    "\n",
    "\n",
    " ## 0. Configuration et Import des Biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Imports pour l'analyse de texte\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Imports pour l'analyse de qualit√©\n",
    "import emoji\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "# Configuration des graphiques\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Biblioth√®ques import√©es avec succ√®s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d14788",
   "metadata": {},
   "source": [
    "## 1. Chargement et Nettoyage Initial des Donn√©es\n",
    "Chargement du CSV et conversion des types de donn√©es (notamment les dates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c75895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donn√©es\n",
    "try:\n",
    "    df = pd.read_csv(r\"D:\\CryptoVibe\\CryptoVibe\\data\\Bronze\\consolidated_data.csv\")\n",
    "    print(f\"Donn√©es charg√©es avec succ√®s: {df.shape[0]} lignes et {df.shape[1]} colonnes.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Erreur: Le fichier 'consolidated_data.csv' n'a pas √©t√© trouv√©.\")\n",
    "    # Cr√©ation d'un DataFrame de test si le fichier n'existe pas\n",
    "    data = {\n",
    "        'id': range(1000),\n",
    "        'text': ['Bitcoin is up!', 'I love #ETH', 'What about $SOL ?', 'This is a test post', 'http://spam.com', 'BTC BTC BTC', 'üöÄüåï', 'DOGE to the moon', 'Buy $SHIB now!', 'This is a very long post designed to test the limits of what a post can be, potentially exceeding the 1000 character limit just for fun.'] * 100,\n",
    "        'date': pd.to_datetime(pd.date_range(start='2023-01-01', periods=1000, freq='H')),\n",
    "        'source': ['twitter', 'reddit'] * 500,\n",
    "        'author': ['user_a', 'user_b', 'user_c', 'user_d', 'user_e'] * 200,\n",
    "        'engagement': np.random.randint(0, 5000, 1000),\n",
    "        'crypto_mentioned': [np.nan] * 1000,\n",
    "        'date_only': pd.to_datetime(pd.date_range(start='2023-01-01', periods=1000, freq='H')).date\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"Un jeu de donn√©es de test a √©t√© cr√©√©.\")\n",
    "\n",
    "# Affichage des premi√®res lignes et des informations\n",
    "print(\"\\n--- Aper√ßu des donn√©es ---\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n--- Informations sur le DataFrame ---\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86463ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Nettoyage Initial et Feature Engineering ---\n",
    "\n",
    "# Conversion de la date (essentiel)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['date_only'] = pd.to_datetime(df['date_only'])\n",
    "\n",
    "# Extraction des composantes temporelles\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['day_of_week'] = df['date'].dt.day_name()\n",
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Nettoyage du texte (remplir les NaNs)\n",
    "df['text'] = df['text'].fillna('')\n",
    "\n",
    "# Calcul de la longueur du texte\n",
    "df['text_length'] = df['text'].str.len()\n",
    "\n",
    "print(\"\\nColonnes temporelles et 'text_length' ajout√©es.\")\n",
    "print(df[['date', 'hour', 'day_of_week', 'text_length']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27557606",
   "metadata": {},
   "source": [
    "## 2.  Statistiques G√©n√©rales\n",
    "Analyse de la taille du dataset, de la distribution des sources, des auteurs et de l'engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bb3194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Statistiques G√©n√©rales ---\n",
    "\n",
    "print(f\"Nombre total de posts: {len(df)}\")\n",
    "print(f\"Plage de dates: Du {df['date'].min()} au {df['date'].max()}\")\n",
    "print(f\"Nombre d'auteurs uniques: {df['author'].nunique()}\")\n",
    "\n",
    "print(\"\\n--- Statistiques d'Engagement ---\")\n",
    "print(df['engagement'].describe(percentiles=[.25, .5, .75]).to_markdown(floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84769005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Distribution par Source (Pie Chart) ---\n",
    "\n",
    "source_counts = df['source'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(source_counts, labels=source_counts.index, autopct='%1.1f%%', startangle=140, colors=['#1DA1F2', '#FF4500'])\n",
    "plt.title('Distribution des Posts par Source (Twitter vs Reddit)')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e92d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Distribution des Posts par Auteur ---\n",
    "\n",
    "author_posts = df['author'].value_counts()\n",
    "power_users = author_posts[author_posts > 50] # Seuil d√©fini dans les requirements (section 5)\n",
    "\n",
    "print(f\"\\nNombre d'auteurs 'Power Users' (> 50 posts): {len(power_users)}\")\n",
    "print(\"Top 10 des 'Power Users':\")\n",
    "print(power_users.head(10).to_markdown())\n",
    "\n",
    "# Visualisation de la distribution (tr√®s asym√©trique, d'o√π l'√©chelle log)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(author_posts, bins=100, log_scale=(False, True))\n",
    "plt.title('Distribution du Nombre de Posts par Auteur')\n",
    "plt.xlabel('Nombre de Posts')\n",
    "plt.ylabel(\"Nombre d'Auteurs (√âchelle Log)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb786bf9",
   "metadata": {},
   "source": [
    "## 3.  Analyse Temporelle\n",
    "Examen de l'√©volution du volume de posts dans le temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07645353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Posts par Jour (Line Chart) ---\n",
    "\n",
    "posts_per_day = df.groupby('date_only').size()\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "posts_per_day.plot(kind='line', label='Posts par Jour')\n",
    "plt.title('Nombre de Posts par Jour au Fil du Temps')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Nombre de Posts')\n",
    "\n",
    "# --- Tendance Temporelle (Regression Line) ---\n",
    "posts_per_day_df = posts_per_day.reset_index(name='count')\n",
    "posts_per_day_df['day_num'] = (posts_per_day_df['date_only'] - posts_per_day_df['date_only'].min()).dt.days\n",
    "\n",
    "sns.regplot(\n",
    "    x='day_num',\n",
    "    y='count',\n",
    "    data=posts_per_day_df,\n",
    "    line_kws={'color':'red', 'linestyle': '--'},\n",
    "    scatter_kws={'alpha':0.3, 's':10},\n",
    "    label='Tendance'\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8397484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Identification des Pics d'Activit√© ---\n",
    "\n",
    "# Un \"pic\" est d√©fini comme un jour d√©passant la moyenne + 2 √©carts-types\n",
    "mean_posts = posts_per_day.mean()\n",
    "std_posts = posts_per_day.std()\n",
    "activity_threshold = mean_posts + (2 * std_posts)\n",
    "\n",
    "activity_spikes = posts_per_day[posts_per_day > activity_threshold]\n",
    "\n",
    "print(f\"Seuil de pic d'activit√© (Moyenne + 2*STD): {activity_threshold:.2f} posts/jour\")\n",
    "print(f\"Jours avec une activit√© de pointe ({len(activity_spikes)} jours):\")\n",
    "print(activity_spikes.sort_values(ascending=False).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833355b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Posts par Heure et Jour de la Semaine ---\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Posts par heure\n",
    "sns.countplot(ax=axes[0], x=df['hour'], palette=\"viridis\")\n",
    "axes[0].set_title('Distribution des Posts par Heure de la Journ√©e')\n",
    "axes[0].set_xlabel('Heure (0-23)')\n",
    "axes[0].set_ylabel('Nombre de Posts')\n",
    "\n",
    "# Posts par jour de la semaine\n",
    "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "sns.countplot(ax=axes[1], x=df['day_of_week'], order=days_order, palette=\"plasma\")\n",
    "axes[1].set_title('Distribution des Posts par Jour de la Semaine')\n",
    "axes[1].set_xlabel('Jour de la Semaine')\n",
    "axes[1].set_ylabel('Nombre de Posts')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d3ebf",
   "metadata": {},
   "source": [
    "## 4.  Analyse du Texte\n",
    "Analyse de la longueur des textes, des mots et n-grams les plus fr√©quents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Distribution de la Longueur du Texte ---\n",
    "\n",
    "print(\"--- Statistiques de la Longueur du Texte ---\")\n",
    "print(df['text_length'].describe(percentiles=[.25, .5, .75]).to_markdown(floatfmt=\".2f\"))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df['text_length'], bins=100, kde=True)\n",
    "plt.title('Distribution de la Longueur du Texte')\n",
    "plt.xlabel('Longueur du Texte (caract√®res)')\n",
    "plt.ylabel('Fr√©quence')\n",
    "plt.xlim(0, 1000) # Limite pour une meilleure lisibilit√©\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a0d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Posts Courts et Longs ---\n",
    "\n",
    "short_posts_count = (df['text_length'] < 10).sum()\n",
    "long_posts_count = (df['text_length'] > 1000).sum()\n",
    "\n",
    "print(f\"Posts trop courts (< 10 caract√®res): {short_posts_count} ({short_posts_count / len(df) * 100:.2f}%)\")\n",
    "print(f\"Posts trop longs (> 1000 caract√®res): {long_posts_count} ({long_posts_count / len(df) * 100:.2f}%)\")\n",
    "\n",
    "if short_posts_count > 0:\n",
    "    print(\"\\nExemples de posts courts:\")\n",
    "    print(df[df['text_length'] < 10]['text'].value_counts().head().to_markdown())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e7d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pr√©paration pour l'analyse des mots (N-grams) ---\n",
    "\n",
    "# T√©l√©chargement des stop words (√† faire une fois)\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Liste des stop words (anglais)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Ajout de termes crypto courants ou de bruit pour les exclure de l'analyse g√©n√©rale\n",
    "custom_stopwords = ['crypto', 'bitcoin', 'eth', 'btc', 'https', 'co', 'com', 'www', 't', 's', 'http', 'rt']\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "# Fonction pour obtenir les top N-grams\n",
    "def get_top_ngrams(corpus, ngram_range=(1, 1), n=20):\n",
    "    vec = CountVectorizer(\n",
    "        stop_words=list(stop_words),\n",
    "        ngram_range=ngram_range,\n",
    "        token_pattern=r'\\b[a-zA-Z]{2,}\\b' # Mots d'au moins 2 lettres\n",
    "    ).fit(corpus)\n",
    "    \n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252cf332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Top 20 Mots (Unigrams) ---\n",
    "\n",
    "top_unigrams = get_top_ngrams(df['text'], ngram_range=(1, 1), n=20)\n",
    "df_top_unigrams = pd.DataFrame(top_unigrams, columns=['Mot', 'Fr√©quence'])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Fr√©quence', y='Mot', data=df_top_unigrams, palette='coolwarm')\n",
    "plt.title('Top 20 des Mots les Plus Fr√©quents (Unigrams)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3577167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Top 20 Bigrams et Trigrams ---\n",
    "\n",
    "top_bigrams = get_top_ngrams(df['text'], ngram_range=(2, 2), n=20)\n",
    "df_top_bigrams = pd.DataFrame(top_bigrams, columns=['Bigram', 'Fr√©quence'])\n",
    "\n",
    "top_trigrams = get_top_ngrams(df['text'], ngram_range=(3, 3), n=20)\n",
    "df_top_trigrams = pd.DataFrame(top_trigrams, columns=['Trigram', 'Fr√©quence'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Plot Bigrams\n",
    "sns.barplot(ax=axes[0], x='Fr√©quence', y='Bigram', data=df_top_bigrams, palette='Greens_d')\n",
    "axes[0].set_title('Top 20 des Bigrams les Plus Fr√©quents')\n",
    "\n",
    "# Plot Trigrams\n",
    "sns.barplot(ax=axes[1], x='Fr√©quence', y='Trigram', data=df_top_trigrams, palette='Blues_d')\n",
    "axes[1].set_title('Top 20 des Trigrams les Plus Fr√©quents')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8126f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Word Cloud ---\n",
    "\n",
    "text_corpus = \" \".join(text for text in df['text'])\n",
    "wordcloud = WordCloud(stopwords=stop_words, background_color=\"white\", width=1000, height=500, max_words=150).generate(text_corpus)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Word Cloud du Vocabulaire Complet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4533dd2",
   "metadata": {},
   "source": [
    "## 5.  D√©tection Crypto (Pr√©liminaire)\n",
    "Comptage manuel des mentions de cryptomonnaies populaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c801478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition des termes √† rechercher (expressions r√©guli√®res non sensibles √† la casse)\n",
    "crypto_terms_map = {\n",
    "    'Bitcoin': r'\\b(bitcoin|btc|‚Çø)\\b',\n",
    "    'Ethereum': r'\\b(ethereum|eth)\\b',\n",
    "    'Solana': r'\\b(solana|sol)\\b',\n",
    "    'Binance Coin': r'\\b(binance|bnb)\\b',\n",
    "    'Cardano': r'\\b(cardano|ada)\\b',\n",
    "    'Ripple': r'\\b(ripple|xrp)\\b',\n",
    "    'Dogecoin': r'\\b(dogecoin|doge)\\b',\n",
    "    'Shiba Inu': r'\\b(shiba|shib)\\b',\n",
    "    'Polygon': r'\\b(polygon|matic)\\b',\n",
    "    'Litecoin': r'\\b(litecoin|ltc)\\b',\n",
    "}\n",
    "\n",
    "crypto_counts = {}\n",
    "df['has_crypto'] = False\n",
    "text_lower = df['text'].str.lower() # Optimisation: mettre en minuscule une seule fois\n",
    "\n",
    "for name, pattern in crypto_terms_map.items():\n",
    "    mentions = text_lower.str.contains(pattern, regex=True, case=False)\n",
    "    crypto_counts[name] = mentions.sum()\n",
    "    df['has_crypto'] = df['has_crypto'] | mentions # Marque True si au moins une crypto est trouv√©e\n",
    "\n",
    "# Dataframe des comptes\n",
    "df_crypto_counts = pd.DataFrame.from_dict(crypto_counts, orient='index', columns=['Mentions'])\n",
    "df_crypto_counts = df_crypto_counts.sort_values('Mentions', ascending=False)\n",
    "\n",
    "print(\"--- Mentions des Top Cryptos ---\")\n",
    "print(df_crypto_counts.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2782e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bar Chart des Top Cryptos ---\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=df_crypto_counts['Mentions'], y=df_crypto_counts.index, palette=\"rocket\")\n",
    "plt.title('Top 10 des Cryptos Mentionn√©es')\n",
    "plt.xlabel('Nombre de Mentions')\n",
    "plt.ylabel('Cryptomonnaie')\n",
    "plt.show()\n",
    "\n",
    "# --- Pourcentage de posts avec mention ---\n",
    "percent_with_crypto = df['has_crypto'].mean() * 100\n",
    "print(f\"\\n{percent_with_crypto:.2f}% des posts mentionnent au moins une crypto de la liste.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d3fec5",
   "metadata": {},
   "source": [
    "## 6.  Analyse de Qualit√© et D√©tection de Spam\n",
    "Identification des probl√®mes potentiels dans les donn√©es : URLs, emojis, doublons, spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60feb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Posts avec URLs ---\n",
    "\n",
    "df['has_url'] = df['text'].str.contains(r'http\\S+|www\\.\\S+', regex=True)\n",
    "url_counts = df['has_url'].value_counts()\n",
    "\n",
    "print(f\"Posts avec URL: {url_counts.get(True, 0)}\")\n",
    "print(f\"Posts sans URL: {url_counts.get(False, 0)}\")\n",
    "\n",
    "sns.barplot(x=url_counts.index, y=url_counts.values)\n",
    "plt.title('Proportion de Posts Contenant une URL')\n",
    "plt.xlabel('Contient une URL')\n",
    "plt.ylabel('Nombre de Posts')\n",
    "plt.xticks([0, 1], ['Non', 'Oui'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d932b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyse des Emojis ---\n",
    "\n",
    "def extract_emojis(text):\n",
    "    return [char for char in text if emoji.is_emoji(char)]\n",
    "\n",
    "# Ceci peut √™tre lent sur un gros dataset\n",
    "# Cr√©ation d'une liste compl√®te de tous les emojis\n",
    "try:\n",
    "    all_emojis_list = df['text'].apply(extract_emojis).sum()\n",
    "    emoji_counts = Counter(all_emojis_list)\n",
    "    df_top_emojis = pd.DataFrame(emoji_counts.most_common(10), columns=['Emoji', 'Compte'])\n",
    "    \n",
    "    print(\"--- Top 10 Emojis ---\")\n",
    "    print(df_top_emojis.to_markdown(index=False))\n",
    "\n",
    "    # Proportion de posts avec emojis\n",
    "    df['has_emoji'] = df['text'].apply(lambda x: len(extract_emojis(x)) > 0)\n",
    "    print(f\"\\nProportion de posts avec au moins 1 emoji: {df['has_emoji'].mean() * 100:.2f}%\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de l'analyse des emojis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a89b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- D√©tection de la Langue ---\n",
    "\n",
    "# AVERTISSEMENT: C'est TR√àS LENT. Nous utilisons un √©chantillon.\n",
    "print(\"\\n--- D√©tection de la langue (sur un √©chantillon de 1000 posts) ---\")\n",
    "\n",
    "sample_size = min(1000, len(df))\n",
    "df_sample = df.sample(sample_size, random_state=42)\n",
    "\n",
    "def detect_lang_safe(text):\n",
    "    if not text or not text.strip():\n",
    "        return 'unknown'\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return 'error'\n",
    "\n",
    "df_sample['language'] = df_sample['text'].apply(detect_lang_safe)\n",
    "lang_distribution = df_sample['language'].value_counts(normalize=True).head(5)\n",
    "\n",
    "print(lang_distribution.to_markdown(floatfmt=\".2%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc48e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Doublons et Spam ---\n",
    "\n",
    "# Duplicates exacts\n",
    "exact_duplicates = df.duplicated(subset=['text']).sum()\n",
    "print(f\"\\nNombre de posts avec texte exact en double: {exact_duplicates} ({exact_duplicates / len(df) * 100:.2f}%)\")\n",
    "\n",
    "# Texte r√©p√©t√© (Spam)\n",
    "repeated_text_counts = df['text'].value_counts()\n",
    "spammy_text = repeated_text_counts[repeated_text_counts > 5]\n",
    "print(f\"\\nNombre de textes uniques r√©p√©t√©s plus de 5 fois: {len(spammy_text)}\")\n",
    "if len(spammy_text) > 0:\n",
    "    print(\"Exemples de textes 'spammy':\")\n",
    "    print(spammy_text.head(5).to_markdown())\n",
    "\n",
    "# Auteurs suspects (identifi√©s comme 'power_users' pr√©c√©demment)\n",
    "print(f\"\\nNombre d'auteurs 'suspects' (> 50 posts): {len(power_users)}\")\n",
    "\n",
    "# Texte tout en majuscules\n",
    "df['all_caps'] = df['text'].str.isupper() & (df['text_length'] > 10) # Ignorer les courts\n",
    "all_caps_count = df['all_caps'].sum()\n",
    "print(f\"\\nNombre de posts en majuscules (potentiel 'screaming'): {all_caps_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684bc251",
   "metadata": {},
   "source": [
    " ## 9.  Rapport Final et Recommandations\n",
    "\n",
    " Synth√®se des observations et des actions recommand√©es pour le pr√©-traitement.\n",
    "\n",
    "\n",
    "\n",
    " ### Executive Summary (Mod√®le)\n",
    "\n",
    " L'analyse exploratoire a port√© sur [Total Posts] posts provenant de Reddit et Twitter, couvrant la p√©riode du [Date Min] au [Date Max]. L'activit√© est domin√©e par [Source Majoritaire] ([X.X]%). L'engagement montre une distribution tr√®s asym√©trique, sugg√©rant que quelques posts g√©n√®rent la majorit√© des interactions. L'analyse temporelle r√©v√®le des pics d'activit√© notables autour de [Date des Pics], indiquant potentiellement des √©v√©nements majeurs du march√©. L'analyse de texte montre que les discussions se concentrent sur [Top Mot 1] et [Top Mot 2], avec une forte pr√©valence de discussions autour de [Top Crypto]. Plusieurs probl√®mes de qualit√© des donn√©es ont √©t√© identifi√©s, notamment [X.X]% de doublons et une proportion significative de messages potentiellement \"spammy\".\n",
    "\n",
    " ###  Key Findings (Constatations Cl√©s)\n",
    "\n",
    " * **Volume et Source:** Le dataset contient [Total Posts] posts, avec une r√©partition [X]% Twitter et [Y]% Reddit.\n",
    " * **Activit√© Temporelle:** L'activit√© est la plus forte le [Jour Max] et la plus faible le [Jour Min]. L'heure de pointe se situe autour de [Heure Max] (UTC/Local?).\n",
    " * **Tendance:** La tendance g√©n√©rale des posts est [√† la hausse / √† la baisse / stable] sur la p√©riode analys√©e.\n",
    " * **Contenu:** Les posts sont en moyenne de [Longueur Moyenne] caract√®res. [X]% des posts contiennent des URLs et [Y]% contiennent des emojis (le üöÄ est le plus populaire).\n",
    " * **Sujets:** [Top Crypto 1] et [Top Crypto 2] sont les cryptomonnaies les plus discut√©es. [X.X]% des posts mentionnent au moins une crypto de notre liste.\n",
    " * **Engagement:** L'engagement m√©dian est de [M√©diane Engagement], mais la moyenne est de [Moyenne Engagement], indiquant une forte asym√©trie (skewness). [Source] g√©n√®re en moyenne un engagement plus √©lev√© (ou plus variable).\n",
    " * **Qualit√©:** [X]% des posts sont des doublons exacts. [Y] auteurs sont des \"power users\" (ou spammeurs) avec plus de 50 posts chacun.\n",
    "\n",
    " ###  Data Quality Issues (Probl√®mes de Qualit√©)\n",
    "\n",
    " 1.  **Doublons:** [Nombre de Doublons] posts sont des doublons exacts et devraient √™tre supprim√©s.\n",
    " 2.  **Spam de Texte:** [Nombre de Textes R√©p√©t√©s] textes uniques sont r√©p√©t√©s plus de 5 fois, indiquant un spam de type \"copier-coller\".\n",
    " 3.  **Spam d'Auteurs:** [Nombre de Power Users] auteurs postent de mani√®re excessive et devraient √™tre examin√©s (pourraient √™tre des bots ou des comptes de news l√©gitimes).\n",
    " 4.  **Posts Courts/Vides:** [Nombre de Posts Courts] posts ont moins de 10 caract√®res et n'apportent probablement aucune valeur (par ex. \"ok\", \"lol\").\n",
    " 5.  **Multilingue:** Environ [X]% des posts ne sont pas en anglais (bas√© sur l'√©chantillon).\n",
    "\n",
    " ###  Recommendations for Cleaning (Recommandations pour le Nettoyage)\n",
    "\n",
    " 1.  **Supprimer les doublons:** `df.drop_duplicates(subset=['text'], keep='first', inplace=True)`\n",
    " 2.  **Filtrer les posts courts:** `df = df[df['text_length'] >= 10]`\n",
    " 3.  **Filtrer les langues (si n√©cessaire):** Appliquer la d√©tection de langue √† l'ensemble du dataset (peut √™tre lent) et conserver uniquement l'anglais (`df = df[df['language'] == 'en']`).\n",
    " 4.  **G√©rer le spam:**\n",
    "     * **Texte:** Identifier les textes r√©p√©t√©s > 5 fois et n'en garder qu'une seule instance (ou les marquer).\n",
    "     * **Auteurs:** Envisager de plafonner le nombre de posts par auteur ou de les exclure si l'analyse se concentre sur l'opinion publique g√©n√©rale plut√¥t que sur les \"influenceurs\".\n",
    " 5.  **Normalisation du texte:** Appliquer un pipeline de nettoyage NLP standard (minuscules, suppression des URLs, suppression de la ponctuation, suppression des emojis) avant la mod√©lisation.\n",
    " 6.  **Enrichissement:** Remplir la colonne `crypto_mentioned` en utilisant une liste de regex plus exhaustive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
